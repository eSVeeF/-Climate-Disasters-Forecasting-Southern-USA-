{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:03:47.119982Z",
     "iopub.status.busy": "2024-11-30T22:03:47.119638Z",
     "iopub.status.idle": "2024-11-30T22:03:47.124711Z",
     "shell.execute_reply": "2024-11-30T22:03:47.123797Z",
     "shell.execute_reply.started": "2024-11-30T22:03:47.119941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import utils_data_preprocessing\n",
    "import lstm_classifier\n",
    "import post_inference\n",
    "# Reload the module\n",
    "importlib.reload(utils_data_preprocessing)\n",
    "importlib.reload(lstm_classifier)\n",
    "importlib.reload(post_inference)\n",
    "\n",
    "# Reinitialize the class\n",
    "from utils_data_preprocessing import Utils_data_preprocessing\n",
    "from lstm_classifier import Lstm_classifier\n",
    "from post_inference import Post_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:03:47.126384Z",
     "iopub.status.busy": "2024-11-30T22:03:47.126122Z",
     "iopub.status.idle": "2024-11-30T22:03:47.842685Z",
     "shell.execute_reply": "2024-11-30T22:03:47.841665Z",
     "shell.execute_reply.started": "2024-11-30T22:03:47.126345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reset tensoflow, each time you run, the run's model is save, generate conflits with the code\n",
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()  # Resets the backend state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_DISASTER_TYPES=[\"Flood\"] # Storm, Flood, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:03:47.844747Z",
     "iopub.status.busy": "2024-11-30T22:03:47.844447Z",
     "iopub.status.idle": "2024-11-30T22:03:47.885197Z",
     "shell.execute_reply": "2024-11-30T22:03:47.884565Z",
     "shell.execute_reply.started": "2024-11-30T22:03:47.844721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emdat = pd.read_csv(\"data/emdat_ready.csv\")\n",
    "\n",
    "clean_emdat = Utils_data_preprocessing().clean_emdat(emdat_df=emdat, \n",
    "                                                   minimum_start_year=2010, # disasters previous this year are discarded (minmum: 2010)\n",
    "                                                   accepted_disasters_types=SELECTED_DISASTER_TYPES) # other disasters with types diferent from this are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:03:50.633192Z",
     "iopub.status.busy": "2024-11-30T22:03:50.632866Z",
     "iopub.status.idle": "2024-11-30T22:03:51.344650Z",
     "shell.execute_reply": "2024-11-30T22:03:51.343968Z",
     "shell.execute_reply.started": "2024-11-30T22:03:50.633163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "noaa = pd.read_csv(\"data/meteostat_noaa.csv\")\n",
    "\n",
    "# Renaming specific columns\n",
    "noaa = noaa.rename(columns={\n",
    "    'State': 'state',\n",
    "    'Season': 'season',\n",
    "    'tavg': 'TAVG',\n",
    "    'pres': 'PRES'\n",
    "})\n",
    "# texas -> Texas\n",
    "noaa['state'] = noaa['state'].str.capitalize()\n",
    "# Winter -> winter\n",
    "noaa['season'] = noaa['season'].str.lower()\n",
    "\n",
    "# Replace NAN's by median of numeric columns only. \n",
    "numeric_cols = noaa.select_dtypes(include='number')  # Select only numeric columns\n",
    "noaa[numeric_cols.columns] = numeric_cols.apply(lambda col: col.fillna(col.median()))\n",
    "\n",
    "N_AFTER_DISASTER_DAYS_TO_LABEL = 0 # only the first day of the diaster is labelled no more, RECOMMENDED TO NOT CHANGE THIS\n",
    "noaa_counted = Utils_data_preprocessing().count_diasters_by_day(clean_emdat_df=clean_emdat, \n",
    "                                                              noaa_df=noaa, \n",
    "                                                              n_after_disaster_days_to_label= N_AFTER_DISASTER_DAYS_TO_LABEL) # if the disaster lasts more than n_after_disaster_days_to_label=2 days, \n",
    "                                                                                                                                #only the first n_after_disaster_days_to_label=2 days are counted as disaster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "<br>Choose if want date for an entire state or for an individual station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:03:51.345836Z",
     "iopub.status.busy": "2024-11-30T22:03:51.345603Z",
     "iopub.status.idle": "2024-11-30T22:03:51.349815Z",
     "shell.execute_reply": "2024-11-30T22:03:51.348874Z",
     "shell.execute_reply.started": "2024-11-30T22:03:51.345813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "N_NEXT_DAYS_UNTIL_DISASTER = 20 # if =7 predict if it will ocurr a disaster the next 7 days, if a disaster ocurred the 8th of April, 1st to 7th of April will be also labelled as disaster (1)\n",
    "LENGTHS_DAYS_MA = [10, 20, 40] # the number of days of Moving Averages, 7-day-MA, 21-day-MA. Each one is a new computed variable\n",
    "MAX_LAG_PERIOD = 5 # how many days we are looking back, if =7, we are creating 7new variables with the past values of the last 7 days of EACH variable\n",
    "SELECTED_STATE='Arkansas'  # rest of states are removed. States: 'Arkansas', 'Kansas', 'Texas', 'Oklahoma', 'Louisiana', 'Mississippi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:03:51.360707Z",
     "iopub.status.busy": "2024-11-30T22:03:51.360472Z",
     "iopub.status.idle": "2024-11-30T22:03:54.529568Z",
     "shell.execute_reply": "2024-11-30T22:03:54.528788Z",
     "shell.execute_reply.started": "2024-11-30T22:03:51.360683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_ready_data = Utils_data_preprocessing().prepare_state_version_data_for_model_predict(data=noaa_counted, \n",
    "                                                                selected_state=SELECTED_STATE,\n",
    "                                                                n_next_days_until_disaster=N_NEXT_DAYS_UNTIL_DISASTER,\n",
    "                                                                lengths_days_ma=LENGTHS_DAYS_MA, \n",
    "                                                                max_lag_period=MAX_LAG_PERIOD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the target like this: <br>\n",
    "[0,   1,     1,    1,    1,    1,    1, 1, 1, 1, 1] adjust to <br>\n",
    "[0, 0.14, 0.29, 0.43, 0.57, 0.71, 0.86, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:03:54.724672Z",
     "iopub.status.busy": "2024-11-30T22:03:54.724415Z",
     "iopub.status.idle": "2024-11-30T22:03:54.729031Z",
     "shell.execute_reply": "2024-11-30T22:03:54.728135Z",
     "shell.execute_reply.started": "2024-11-30T22:03:54.724647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL. if you do not want to adjust set = False\n",
    "WANT_TO_ADJUST_TARGET = False\n",
    "if WANT_TO_ADJUST_TARGET:\n",
    "    model_ready_data['target'] = Utils_data_preprocessing().adjust_days_previous_disaster(column_to_adjust=model_ready_data['target'], \n",
    "                                                                                        n_next_days_until_disaster=N_NEXT_DAYS_UNTIL_DISASTER) # IT HAS TO BE THE SAME AS BEFORE (before=the .prepare_ function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:03:54.730272Z",
     "iopub.status.busy": "2024-11-30T22:03:54.730059Z",
     "iopub.status.idle": "2024-11-30T22:03:54.792521Z",
     "shell.execute_reply": "2024-11-30T22:03:54.791819Z",
     "shell.execute_reply.started": "2024-11-30T22:03:54.730250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cols_not_scale = [col for col in model_ready_data.columns \n",
    "                  if 'WT' in col or 'season' in col or 'target' in col]\n",
    "\n",
    "scaled_data = Lstm_classifier().scale_data(data=model_ready_data, \n",
    "                                           choosen_scaler='standard', # 'standard', 'minmax', 'quantile'\n",
    "                                           cols_not_scale=cols_not_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:03:54.793764Z",
     "iopub.status.busy": "2024-11-30T22:03:54.793534Z",
     "iopub.status.idle": "2024-11-30T22:03:54.797629Z",
     "shell.execute_reply": "2024-11-30T22:03:54.796692Z",
     "shell.execute_reply.started": "2024-11-30T22:03:54.793740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the split% to see which gets that the test set is the last SEQUENCE_LENGTH+1 days, so we predict correctly the last days\n",
    "split_ratio_to_predict_real = (model_ready_data.shape[0]-(SEQUENCE_LENGTH+1))/model_ready_data.shape[0]\n",
    "\n",
    "#later do the same for inference_noaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = Lstm_classifier().lstm_time_series_train_test_split(scaled_data=scaled_data,\n",
    "                                                                                    target_column=model_ready_data['target'],  \n",
    "                                                                                    sequence_length=SEQUENCE_LENGTH, \n",
    "                                                                                    train_test_split_ratio=split_ratio_to_predict_real)\n",
    "    \n",
    "# clean previous info (this is for plots)\n",
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()\n",
    "\n",
    "# Build LSTM Model\n",
    "model = Lstm_classifier().train_lstm(X_train=X_train, \n",
    "                                    y_train=y_train, \n",
    "                                    units=100, # more units -> more powerful but more time-comsuming and more risk of overfitting\n",
    "                                    dropout=0.2, # more dropout -> more sleep neurons when traing, less probable to overfit\n",
    "                                    l2_regularizer_weight=0.001, # how much penalty you want to set for large kernel weight in the loss, large pentalty = more likely to use small kernel weights and not overfit. Set to 0.0 if you do not want l2-regularizer\n",
    "                                    learning_rate=0.00001,  # less rate -> more slow when learing, less probable to overfit\n",
    "                                    class_1_weight=1.0, # weight of the 1's class (disaster class), 1.0=same importance as 0's, 5.0=predicting 1's wrong penalizes 5 times more when than 0's. As a reference you can use: class_1_weight=sum(y_train == 0)/sum(y_train == 1)\n",
    "                                    epochs=5, # number of iterations where lstm goes through the entire dataset\n",
    "                                    batch_size=32, # how many samples of data are processed together in a single forward and backward pass of the model. For smaller datasets, smaller batch sizes (e.g., 8, 16, or 32) are usually better to ensure the model doesn’t overfit. For large datasets, larger batch sizes (e.g., 64, 128, 256) can speed up training. \n",
    "                                    validation_data=(X_val, y_val), \n",
    "                                    verbose=0,\n",
    "                                    show_plots=False) # at the end of the training if True, it will plot some monitoring \n",
    "    \n",
    "# predict\n",
    "predictions_probs = model.predict(X_val).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predicted dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the non-selected states\n",
    "noaa_counted_selected_state = noaa_counted[noaa_counted[\"state\"]==SELECTED_STATE]\n",
    "# get the station with the last date\n",
    "station_with_last_date = noaa_counted_selected_state.groupby(\"STATION\")[\"DATE\"].max().idxmax()\n",
    "# get all the dates of that station\n",
    "all_dates = noaa_counted_selected_state[noaa_counted_selected_state[\"STATION\"]==station_with_last_date][\"DATE\"]\n",
    "# get only the predicted ones\n",
    "predicted_dates = all_dates.iloc[-(SEQUENCE_LENGTH+1):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create csv and save predictions and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.DataFrame({\"DATE\":predicted_dates,f\"{SELECTED_STATE}_{SELECTED_DISASTER_TYPES[0]}_pred_prob_next_{SEQUENCE_LENGTH+1}_days\": predictions_probs})\n",
    "pred_data.to_csv(f\"predictions/{SELECTED_STATE}_{SELECTED_DISASTER_TYPES[0]}_pred_prob_next_{SEQUENCE_LENGTH+1}_days.csv\")\n",
    "\n",
    "model.save(f'trained_models/{SELECTED_STATE}_{SELECTED_DISASTER_TYPES[0]}_pred_prob_next_{SEQUENCE_LENGTH+1}_days.keras') "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6199881,
     "sourceId": 10060749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6200122,
     "sourceId": 10061049,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
